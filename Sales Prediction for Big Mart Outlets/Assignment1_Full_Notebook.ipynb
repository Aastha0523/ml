{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fd2797",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Assignment 1: Data Prep Methods & RMSE Impact Study\n",
    "_Outliers · Missingness · Encoding · Transforms · Binning_\n",
    "\n",
    "**Goal:** Demonstrate, **one method per section**, how classic data preparation techniques change downstream **Linear Regression** performance.  \n",
    "After each method:\n",
    "1) Fit stats **on TRAIN only**,  \n",
    "2) Transform **train/valid/test consistently**,  \n",
    "3) Train **LinearRegression**,  \n",
    "4) Print & log **RMSE (train/valid/test)** (+ MAE, R² optional),  \n",
    "5) Append a row to a shared **results tracker**.\n",
    "\n",
    "> ✅ Dataset placeholder: You will **upload / point to a CSV** in the next cell (variable `DATA_CSV_PATH`).  \n",
    "> ✅ Split once: **60/20/20** with fixed `RANDOM_SEED`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f1edc",
   "metadata": {},
   "source": [
    "## 1) Imports, Seeding, Small Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core\n",
    "import math, warnings, sys, os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling & preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Visualization (matplotlib only; no seaborn)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Reproducibility ---\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# --- Small helpers ---\n",
    "def rmse(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    return mean_squared_error(y, preds, squared=False)\n",
    "\n",
    "def mae(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    return mean_absolute_error(y, preds)\n",
    "\n",
    "def r2(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    return r2_score(y, preds)\n",
    "\n",
    "def evaluate_and_log(method_name, Xtr, ytr, Xva, yva, Xte, yte, results_df):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(Xtr, ytr)\n",
    "    row = {\n",
    "        \"method\": method_name,\n",
    "        \"n_features_after_prep\": Xtr.shape[1],\n",
    "        \"rmse_train\": rmse(lr, Xtr, ytr),\n",
    "        \"rmse_valid\": rmse(lr, Xva, yva),\n",
    "        \"rmse_test\":  rmse(lr, Xte, yte),\n",
    "        \"mae_valid\":  mae(lr, Xva, yva),\n",
    "        \"r2_valid\":   r2(lr, Xva, yva),\n",
    "    }\n",
    "    print(row)\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
    "    return results_df, lr\n",
    "\n",
    "def detect_column_types(df, target_col):\n",
    "    # Basic type detection\n",
    "    num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if target_col in num_cols:\n",
    "        num_cols.remove(target_col)\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Try to find datetime-like columns\n",
    "    dt_cols = []\n",
    "    for c in df.columns:\n",
    "        if c == target_col: \n",
    "            continue\n",
    "        if c in num_cols or c in cat_cols:\n",
    "            continue\n",
    "        try:\n",
    "            parsed = pd.to_datetime(df[c], errors='raise', infer_datetime_format=True)\n",
    "            dt_cols.append(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Also scan object columns that might be datetime\n",
    "    for c in list(cat_cols):\n",
    "        try:\n",
    "            parsed = pd.to_datetime(df[c], errors='raise', infer_datetime_format=True)\n",
    "            dt_cols.append(c)\n",
    "            cat_cols.remove(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Deduplicate\n",
    "    dt_cols = list(dict.fromkeys(dt_cols))\n",
    "    # Remove dt from num if mistakenly included\n",
    "    num_cols = [c for c in num_cols if c not in dt_cols]\n",
    "\n",
    "    return num_cols, cat_cols, dt_cols\n",
    "\n",
    "def safe_log1p(x):\n",
    "    # Safely handle non-positive values by shifting\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    minx = np.nanmin(x)\n",
    "    shift = 1.0 - minx if minx <= 0 else 0.0\n",
    "    return np.log1p(x + shift)\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413e10a",
   "metadata": {},
   "source": [
    "## 2) Data Load + Single Split (60/20/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === PLACEHOLDER: point to your CSV here ===\n",
    "# Example: DATA_CSV_PATH = \"BigMart_Train.csv\"\n",
    "DATA_CSV_PATH = \"YOUR_DATASET.csv\"   # <-- Replace with your CSV\n",
    "TARGET_COL = \"y\"                      # <-- Replace with your target column name\n",
    "\n",
    "# You can use a simple file uploader in notebook environments like Colab/JupyterLab:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# DATA_CSV_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(DATA_CSV_PATH)\n",
    "\n",
    "assert TARGET_COL in df.columns, f\"Target column '{TARGET_COL}' not found in CSV. Please set TARGET_COL correctly.\"\n",
    "\n",
    "# --- Optional: Light cleaning of obvious all-NA columns ---\n",
    "all_na_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "if all_na_cols:\n",
    "    print(\"Dropping all-NA columns:\", all_na_cols)\n",
    "    df = df.drop(columns=all_na_cols)\n",
    "\n",
    "# --- Split: 60/20/20 ---\n",
    "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=RANDOM_SEED)\n",
    "df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "ytr, yva, yte = df_train[TARGET_COL], df_valid[TARGET_COL], df_test[TARGET_COL]\n",
    "\n",
    "# Detect column types (will be reused by methods)\n",
    "num_cols, cat_cols, dt_cols = detect_column_types(df, TARGET_COL)\n",
    "\n",
    "print(\"Shapes:\", df_train.shape, df_valid.shape, df_test.shape)\n",
    "print(\"Numeric cols:\", len(num_cols), \"| Categorical cols:\", len(cat_cols), \"| Datetime-like cols:\", len(dt_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c58ab",
   "metadata": {},
   "source": [
    "## 3) Baseline: Raw Linear Regression (no scaling/encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only numeric features available as-is (no encoding/scaling)\n",
    "Xtr_base = df_train[num_cols].copy()\n",
    "Xva_base = df_valid[num_cols].copy()\n",
    "Xte_base = df_test[num_cols].copy()\n",
    "\n",
    "# Drop columns with any missing values (raw baseline, strict)\n",
    "cols_no_na = [c for c in num_cols if df_train[c].notna().all() and df_valid[c].notna().all() and df_test[c].notna().all()]\n",
    "Xtr_b = Xtr_base[cols_no_na]\n",
    "Xva_b = Xva_base[cols_no_na]\n",
    "Xte_b = Xte_base[cols_no_na]\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"method\",\"n_features_after_prep\",\"rmse_train\",\"rmse_valid\",\"rmse_test\",\"mae_valid\",\"r2_valid\"])\n",
    "results_df, _ = evaluate_and_log(\"Baseline (raw LR, numeric only, drop-NA cols)\", Xtr_b, ytr, Xva_b, yva, Xte_b, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280cd00",
   "metadata": {},
   "source": [
    "## 4) Outlier Handling — Visual Inspection (Boxplot / Scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f00333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visuals for a couple of numeric columns (picked heuristically: top-variance features)\n",
    "# NOTE: This is *exploratory* (no transformation). Add your brief comments after viewing.\n",
    "if len(num_cols) > 0:\n",
    "    variances = df_train[num_cols].var().sort_values(ascending=False)\n",
    "    top_cols = variances.index[:2].tolist()\n",
    "    for c in top_cols:\n",
    "        plt.figure()\n",
    "        df_train[c].plot(kind='box', title=f\"Boxplot: {c}\")\n",
    "        plt.show()\n",
    "\n",
    "    if len(top_cols) == 2:\n",
    "        plt.figure()\n",
    "        plt.scatter(df_train[top_cols[0]], df_train[top_cols[1]])\n",
    "        plt.title(f\"Scatter: {top_cols[0]} vs {top_cols[1]}\")\n",
    "        plt.xlabel(top_cols[0]); plt.ylabel(top_cols[1])\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No numeric columns available for visual inspection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabfdf6",
   "metadata": {},
   "source": [
    "### Z-Score Rule (cap outliers at ±3σ) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_zscore_params(df, numeric_cols):\n",
    "    means = df[numeric_cols].mean()\n",
    "    stds = df[numeric_cols].std().replace(0, np.nan)\n",
    "    return means, stds\n",
    "\n",
    "def apply_zscore_cap(df, numeric_cols, means, stds, z=3.0):\n",
    "    out = df.copy()\n",
    "    for c in numeric_cols:\n",
    "        m, s = means[c], stds[c]\n",
    "        if pd.isna(s) or s == 0:\n",
    "            continue\n",
    "        lower, upper = m - z*s, m + z*s\n",
    "        out[c] = out[c].clip(lower, upper)\n",
    "    return out\n",
    "\n",
    "means, stds = fit_zscore_params(df_train, num_cols)\n",
    "Xtr = apply_zscore_cap(df_train[num_cols], num_cols, means, stds, z=3.0)\n",
    "Xva = apply_zscore_cap(df_valid[num_cols], num_cols, means, stds, z=3.0)\n",
    "Xte = apply_zscore_cap(df_test[num_cols],  num_cols, means, stds, z=3.0)\n",
    "\n",
    "# Fill remaining NA with train mean (to allow LR)\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "Xtr_i = pd.DataFrame(mean_imputer.fit_transform(Xtr), columns=num_cols, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(mean_imputer.transform(Xva),   columns=num_cols, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(mean_imputer.transform(Xte),   columns=num_cols, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Outlier: Z-Score cap (±3σ) + mean impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5c739",
   "metadata": {},
   "source": [
    "### IQR Rule (cap at [Q1-1.5*IQR, Q3+1.5*IQR]) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_iqr_params(df, numeric_cols):\n",
    "    Q1 = df[numeric_cols].quantile(0.25)\n",
    "    Q3 = df[numeric_cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return Q1, Q3, IQR\n",
    "\n",
    "def apply_iqr_cap(df, numeric_cols, Q1, Q3, IQR, whisk=1.5):\n",
    "    out = df.copy()\n",
    "    lower = Q1 - whisk * IQR\n",
    "    upper = Q3 + whisk * IQR\n",
    "    for c in numeric_cols:\n",
    "        out[c] = out[c].clip(lower[c], upper[c])\n",
    "    return out\n",
    "\n",
    "Q1, Q3, IQR = fit_iqr_params(df_train, num_cols)\n",
    "Xtr = apply_iqr_cap(df_train[num_cols], num_cols, Q1, Q3, IQR, 1.5)\n",
    "Xva = apply_iqr_cap(df_valid[num_cols], num_cols, Q1, Q3, IQR, 1.5)\n",
    "Xte = apply_iqr_cap(df_test[num_cols],  num_cols, Q1, Q3, IQR, 1.5)\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "Xtr_i = pd.DataFrame(mean_imputer.fit_transform(Xtr), columns=num_cols, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(mean_imputer.transform(Xva),   columns=num_cols, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(mean_imputer.transform(Xte),   columns=num_cols, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Outlier: IQR cap (1.5*IQR) + mean impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad460d1",
   "metadata": {},
   "source": [
    "### Percentile Capping (Winsorization at [1%, 99%]) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c356461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_percentiles(df, numeric_cols, low=0.01, high=0.99):\n",
    "    lows = df[numeric_cols].quantile(low)\n",
    "    highs = df[numeric_cols].quantile(high)\n",
    "    return lows, highs\n",
    "\n",
    "def apply_percentile_cap(df, numeric_cols, lows, highs):\n",
    "    out = df.copy()\n",
    "    for c in numeric_cols:\n",
    "        out[c] = out[c].clip(lows[c], highs[c])\n",
    "    return out\n",
    "\n",
    "lows, highs = fit_percentiles(df_train, num_cols, 0.01, 0.99)\n",
    "Xtr = apply_percentile_cap(df_train[num_cols], num_cols, lows, highs)\n",
    "Xva = apply_percentile_cap(df_valid[num_cols], num_cols, lows, highs)\n",
    "Xte = apply_percentile_cap(df_test[num_cols],  num_cols, lows, highs)\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "Xtr_i = pd.DataFrame(mean_imputer.fit_transform(Xtr), columns=num_cols, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(mean_imputer.transform(Xva),   columns=num_cols, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(mean_imputer.transform(Xte),   columns=num_cols, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Outlier: Percentile cap [1%,99%] + mean impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec06e2",
   "metadata": {},
   "source": [
    "## 5) Missing Value Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1af94",
   "metadata": {},
   "source": [
    "### Listwise Deletion (drop rows with any NA in numeric) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr = df_train[num_cols].dropna(axis=0)\n",
    "ytr_ld = ytr.loc[Xtr.index]\n",
    "Xva = df_valid[num_cols].dropna(axis=0)\n",
    "yva_ld = yva.loc[Xva.index]\n",
    "Xte = df_test[num_cols].dropna(axis=0)\n",
    "yte_ld = yte.loc[Xte.index]\n",
    "\n",
    "# Note: Different sizes; evaluating on reduced sets\n",
    "results_df, _ = evaluate_and_log(\"Missing: Listwise deletion (numeric)\", Xtr, ytr_ld, Xva, yva_ld, Xte, yte_ld, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714d954",
   "metadata": {},
   "source": [
    "### Drop Columns (missingness threshold 40%) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.4\n",
    "miss_ratio = df_train[num_cols].isna().mean()\n",
    "keep_cols = miss_ratio[miss_ratio <= threshold].index.tolist()\n",
    "\n",
    "Xtr = df_train[keep_cols].copy()\n",
    "Xva = df_valid[keep_cols].copy()\n",
    "Xte = df_test[keep_cols].copy()\n",
    "\n",
    "# Remaining NA -> mean impute\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=keep_cols, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(imp.transform(Xva),   columns=keep_cols, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(imp.transform(Xte),   columns=keep_cols, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Missing: Drop columns (≤40% NA) + mean impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd130d",
   "metadata": {},
   "source": [
    "### Mean / Median / Mode Imputation -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric: mean, Categorical: most_frequent\n",
    "Xtr_num = df_train[num_cols].copy()\n",
    "Xva_num = df_valid[num_cols].copy()\n",
    "Xte_num = df_test[num_cols].copy()\n",
    "\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "Xtr_num_m = pd.DataFrame(imp_mean.fit_transform(Xtr_num), columns=num_cols, index=Xtr_num.index)\n",
    "Xva_num_m = pd.DataFrame(imp_mean.transform(Xva_num),   columns=num_cols, index=Xva_num.index)\n",
    "Xte_num_m = pd.DataFrame(imp_mean.transform(Xte_num),   columns=num_cols, index=Xte_num.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Missing: Mean Imputation (numeric only)\", Xtr_num_m, ytr, Xva_num_m, yva, Xte_num_m, yte, results_df)\n",
    "\n",
    "# Median\n",
    "imp_med = SimpleImputer(strategy='median')\n",
    "Xtr_num_med = pd.DataFrame(imp_med.fit_transform(Xtr_num), columns=num_cols, index=Xtr_num.index)\n",
    "Xva_num_med = pd.DataFrame(imp_med.transform(Xva_num),   columns=num_cols, index=Xva_num.index)\n",
    "Xte_num_med = pd.DataFrame(imp_med.transform(Xte_num),   columns=num_cols, index=Xte_num.index)\n",
    "results_df, _ = evaluate_and_log(\"Missing: Median Imputation (numeric only)\", Xtr_num_med, ytr, Xva_num_med, yva, Xte_num_med, yte, results_df)\n",
    "\n",
    "# If you want to include categorical into LR, simple one-hot after mode impute:\n",
    "if len(cat_cols) > 0:\n",
    "    imp_mode = SimpleImputer(strategy='most_frequent')\n",
    "    tr_cat = pd.DataFrame(imp_mode.fit_transform(df_train[cat_cols]), columns=cat_cols, index=df_train.index)\n",
    "    va_cat = pd.DataFrame(imp_mode.transform(df_valid[cat_cols]),   columns=cat_cols, index=df_valid.index)\n",
    "    te_cat = pd.DataFrame(imp_mode.transform(df_test[cat_cols]),    columns=cat_cols, index=df_test.index)\n",
    "\n",
    "    # One-hot\n",
    "    tr_all = pd.concat([Xtr_num_m, tr_cat], axis=1)\n",
    "    va_all = pd.concat([Xva_num_m, va_cat], axis=1)\n",
    "    te_all = pd.concat([Xte_num_m, te_cat], axis=1)\n",
    "\n",
    "    tr_all = pd.get_dummies(tr_all, drop_first=False)\n",
    "    va_all = pd.get_dummies(va_all, drop_first=False)\n",
    "    te_all = pd.get_dummies(te_all, drop_first=False)\n",
    "\n",
    "    # Align columns\n",
    "    va_all = va_all.reindex(columns=tr_all.columns, fill_value=0)\n",
    "    te_all = te_all.reindex(columns=tr_all.columns, fill_value=0)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Missing: Mean(num)+Mode(cat) + One-Hot\", tr_all, ytr, va_all, yva, te_all, yte, results_df)\n",
    "else:\n",
    "    print(\"No categorical columns for mode imputation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93db57",
   "metadata": {},
   "source": [
    "### Constant Value Imputation (+ was_missing flag) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "const_val = -999.0\n",
    "Xtr = df_train[num_cols].copy()\n",
    "Xva = df_valid[num_cols].copy()\n",
    "Xte = df_test[num_cols].copy()\n",
    "\n",
    "# Add missing flags\n",
    "for c in num_cols:\n",
    "    Xtr[f\"{c}__was_missing\"] = Xtr[c].isna().astype(int)\n",
    "    Xva[f\"{c}__was_missing\"] = Xva[c].isna().astype(int)\n",
    "    Xte[f\"{c}__was_missing\"] = Xte[c].isna().astype(int)\n",
    "\n",
    "imp_const = SimpleImputer(strategy='constant', fill_value=const_val)\n",
    "Xtr_i = pd.DataFrame(imp_const.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(imp_const.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(imp_const.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Missing: Constant Impute (-999) + flags\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb10fe0",
   "metadata": {},
   "source": [
    "### k-NN Imputation (numeric) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9437f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(num_cols) > 0:\n",
    "    knn = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "    Xtr = pd.DataFrame(knn.fit_transform(df_train[num_cols]), columns=num_cols, index=df_train.index)\n",
    "    Xva = pd.DataFrame(knn.transform(df_valid[num_cols]),    columns=num_cols, index=df_valid.index)\n",
    "    Xte = pd.DataFrame(knn.transform(df_test[num_cols]),     columns=num_cols, index=df_test.index)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Missing: KNNImputer(k=5)\", Xtr, ytr, Xva, yva, Xte, yte, results_df)\n",
    "else:\n",
    "    print(\"No numeric columns for KNNImputer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279ea5b",
   "metadata": {},
   "source": [
    "## 6) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e79b8",
   "metadata": {},
   "source": [
    "### New Feature Creation (ratios / simple math) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea64ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr = df_train[num_cols].copy()\n",
    "Xva = df_valid[num_cols].copy()\n",
    "Xte = df_test[num_cols].copy()\n",
    "\n",
    "# Example: create up to 3 synthetic features based on top-variance columns\n",
    "if len(num_cols) >= 2:\n",
    "    top = df_train[num_cols].var().sort_values(ascending=False).index.tolist()[:3]\n",
    "    # Avoid division by zero\n",
    "    for i in range(len(top)-1):\n",
    "        a, b = top[i], top[i+1]\n",
    "        Xtr[f\"{a}_over_{b}\"] = (df_train[a] / (df_train[b].replace(0, np.nan))).replace([np.inf, -np.inf], np.nan)\n",
    "        Xva[f\"{a}_over_{b}\"] = (df_valid[a] / (df_valid[b].replace(0, np.nan))).replace([np.inf, -np.inf], np.nan)\n",
    "        Xte[f\"{a}_over_{b}\"] = (df_test[a] / (df_test[b].replace(0, np.nan))).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute NA created by divisions\n",
    "imp = SimpleImputer(strategy='median')\n",
    "Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"FE: Simple ratios on top-variance features + median impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17d5ca",
   "metadata": {},
   "source": [
    "### Feature Interactions (pairwise product of top-2) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23100e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr = df_train[num_cols].copy()\n",
    "Xva = df_valid[num_cols].copy()\n",
    "Xte = df_test[num_cols].copy()\n",
    "\n",
    "if len(num_cols) >= 2:\n",
    "    top2 = df_train[num_cols].var().sort_values(ascending=False).index.tolist()[:2]\n",
    "    c1, c2 = top2[0], top2[1]\n",
    "    Xtr[f\"{c1}_x_{c2}\"] = df_train[c1] * df_train[c2]\n",
    "    Xva[f\"{c1}_x_{c2}\"] = df_valid[c1] * df_valid[c2]\n",
    "    Xte[f\"{c1}_x_{c2}\"] = df_test[c1] * df_test[c2]\n",
    "\n",
    "imp = SimpleImputer(strategy='median')\n",
    "Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"FE: Interaction (top-2 product) + median impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf313fb",
   "metadata": {},
   "source": [
    "### Date/Time Extraction (Y, M, D, DoW) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_date_parts(df, date_cols):\n",
    "    out = df.copy()\n",
    "    made_any = False\n",
    "    for c in date_cols:\n",
    "        try:\n",
    "            dt = pd.to_datetime(out[c], errors='raise', infer_datetime_format=True)\n",
    "            out[f\"{c}_year\"]  = dt.dt.year\n",
    "            out[f\"{c}_month\"] = dt.dt.month\n",
    "            out[f\"{c}_day\"]   = dt.dt.day\n",
    "            out[f\"{c}_dow\"]   = dt.dt.dayofweek\n",
    "            made_any = True\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out, made_any\n",
    "\n",
    "Xtr = df_train.drop(columns=[TARGET_COL], errors='ignore').copy()\n",
    "Xva = df_valid.drop(columns=[TARGET_COL], errors='ignore').copy()\n",
    "Xte = df_test.drop(columns=[TARGET_COL], errors='ignore').copy()\n",
    "\n",
    "Xtr, made_tr = extract_date_parts(Xtr, dt_cols)\n",
    "Xva, made_va = extract_date_parts(Xva, dt_cols)\n",
    "Xte, made_te = extract_date_parts(Xte, dt_cols)\n",
    "\n",
    "# Keep numeric only for LR unless encoded\n",
    "Xtr_num = Xtr.select_dtypes(include=['number'])\n",
    "Xva_num = Xva.select_dtypes(include=['number'])\n",
    "Xte_num = Xte.select_dtypes(include=['number'])\n",
    "\n",
    "imp = SimpleImputer(strategy='median')\n",
    "Xtr_i = pd.DataFrame(imp.fit_transform(Xtr_num), columns=Xtr_num.columns, index=Xtr_num.index)\n",
    "Xva_i = pd.DataFrame(imp.transform(Xva_num),   columns=Xva_num.columns, index=Xva_num.index)\n",
    "Xte_i = pd.DataFrame(imp.transform(Xte_num),   columns=Xte_num.columns, index=Xte_num.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"FE: Date parts extracted + median impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee1ddf",
   "metadata": {},
   "source": [
    "### Aggregations (group-based mean target by a categorical) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a categorical column with many repeats; use mean target per category as a new feature.\n",
    "if len(cat_cols) > 0:\n",
    "    # Pick the categorical with highest frequency average\n",
    "    cardinality = {c: df_train[c].nunique() for c in cat_cols}\n",
    "    cat_chosen = sorted(cardinality, key=cardinality.get)[0]\n",
    "\n",
    "    # Fit on train: mean target by category\n",
    "    grp = df_train.groupby(cat_chosen)[TARGET_COL].mean().to_dict()\n",
    "\n",
    "    def map_mean(df, col, mapping, default=np.nan):\n",
    "        return df[col].map(mapping).fillna(default)\n",
    "\n",
    "    Xtr = df_train[num_cols].copy()\n",
    "    Xva = df_valid[num_cols].copy()\n",
    "    Xte = df_test[num_cols].copy()\n",
    "\n",
    "    Xtr[f\"{cat_chosen}__mean_y\"] = map_mean(df_train, cat_chosen, grp)\n",
    "    Xva[f\"{cat_chosen}__mean_y\"] = map_mean(df_valid, cat_chosen, grp)\n",
    "    Xte[f\"{cat_chosen}__mean_y\"] = map_mean(df_test,  cat_chosen, grp)\n",
    "\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "    Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(f\"FE: Aggregation mean(target) by {cat_chosen}\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n",
    "else:\n",
    "    print(\"No categorical columns available for aggregation-based FE.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad729210",
   "metadata": {},
   "source": [
    "## 7) Encoding (Categoricals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c85359",
   "metadata": {},
   "source": [
    "### Label Encoding -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122878f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(cat_cols) > 0:\n",
    "    Xtr = df_train[num_cols].copy()\n",
    "    Xva = df_valid[num_cols].copy()\n",
    "    Xte = df_test[num_cols].copy()\n",
    "\n",
    "    # Label encode by factorize (learn on train -> mapping)\n",
    "    for c in cat_cols:\n",
    "        vals = pd.Index(df_train[c].astype('category').cat.categories)\n",
    "        mapping = {k:i for i,k in enumerate(vals)}\n",
    "        # Fallback for unseen\n",
    "        Xtr[c] = df_train[c].map(mapping).fillna(-1).astype(int)\n",
    "        Xva[c] = df_valid[c].map(mapping).fillna(-1).astype(int)\n",
    "        Xte[c] = df_test[c].map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "    # Impute numeric NAs if any\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "    Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Encode: Label Encoding (factorize)\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n",
    "else:\n",
    "    print(\"No categorical columns for Label Encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f05749",
   "metadata": {},
   "source": [
    "### One-Hot Encoding -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5419fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(cat_cols) > 0:\n",
    "    Xtr = pd.concat([df_train[num_cols], df_train[cat_cols]], axis=1)\n",
    "    Xva = pd.concat([df_valid[num_cols], df_valid[cat_cols]], axis=1)\n",
    "    Xte = pd.concat([df_test[num_cols],  df_test[cat_cols]],  axis=1)\n",
    "\n",
    "    # Impute numeric + categorical separately before one-hot\n",
    "    num_imp = SimpleImputer(strategy='median')\n",
    "    cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    Xtr_num = pd.DataFrame(num_imp.fit_transform(Xtr[num_cols]), columns=num_cols, index=Xtr.index)\n",
    "    Xva_num = pd.DataFrame(num_imp.transform(Xva[num_cols]),   columns=num_cols, index=Xva.index)\n",
    "    Xte_num = pd.DataFrame(num_imp.transform(Xte[num_cols]),   columns=num_cols, index=Xte.index)\n",
    "\n",
    "    Xtr_cat = pd.DataFrame(cat_imp.fit_transform(Xtr[cat_cols]), columns=cat_cols, index=Xtr.index)\n",
    "    Xva_cat = pd.DataFrame(cat_imp.transform(Xva[cat_cols]),   columns=cat_cols, index=Xva.index)\n",
    "    Xte_cat = pd.DataFrame(cat_imp.transform(Xte[cat_cols]),   columns=cat_cols, index=Xte.index)\n",
    "\n",
    "    Xtr_all = pd.get_dummies(pd.concat([Xtr_num, Xtr_cat], axis=1), drop_first=False)\n",
    "    Xva_all = pd.get_dummies(pd.concat([Xva_num, Xva_cat], axis=1), drop_first=False)\n",
    "    Xte_all = pd.get_dummies(pd.concat([Xte_num, Xte_cat], axis=1), drop_first=False)\n",
    "\n",
    "    Xva_all = Xva_all.reindex(columns=Xtr_all.columns, fill_value=0)\n",
    "    Xte_all = Xte_all.reindex(columns=Xtr_all.columns, fill_value=0)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Encode: One-Hot (num median + cat mode impute)\", Xtr_all, ytr, Xva_all, yva, Xte_all, yte, results_df)\n",
    "else:\n",
    "    print(\"No categorical columns for One-Hot Encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccad2eb",
   "metadata": {},
   "source": [
    "### Ordinal Encoding (by frequency) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9daf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(cat_cols) > 0:\n",
    "    Xtr = df_train[num_cols].copy()\n",
    "    Xva = df_valid[num_cols].copy()\n",
    "    Xte = df_test[num_cols].copy()\n",
    "\n",
    "    for c in cat_cols:\n",
    "        freq = df_train[c].value_counts()\n",
    "        order = {k:i for i,(k,_) in enumerate(freq.items())}\n",
    "        Xtr[c] = df_train[c].map(order).fillna(-1).astype(int)\n",
    "        Xva[c] = df_valid[c].map(order).fillna(-1).astype(int)\n",
    "        Xte[c] = df_test[c].map(order).fillna(-1).astype(int)\n",
    "\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "    Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Encode: Ordinal by frequency\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n",
    "else:\n",
    "    print(\"No categorical columns for Ordinal Encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72d53d",
   "metadata": {},
   "source": [
    "### Target / Mean Encoding (Out-of-Fold) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def target_mean_encode_oof(train_df, valid_df, test_df, cat_cols, target, n_splits=5, noise_std=0.0):\n",
    "    Xtr = train_df.copy()\n",
    "    Xva = valid_df.copy()\n",
    "    Xte = test_df.copy()\n",
    "\n",
    "    global_mean = Xtr[target].mean()\n",
    "\n",
    "    for c in cat_cols:\n",
    "        Xtr[f\"{c}__te\"] = np.nan\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "        for tr_idx, te_idx in kf.split(Xtr):\n",
    "            tr_fold = Xtr.iloc[tr_idx]\n",
    "            te_fold = Xtr.iloc[te_idx]\n",
    "            m = tr_fold.groupby(c)[target].mean()\n",
    "            Xtr.loc[te_idx, f\"{c}__te\"] = te_fold[c].map(m)\n",
    "\n",
    "        # Fill unseen with global mean\n",
    "        Xtr[f\"{c}__te\"] = Xtr[f\"{c}__te\"].fillna(global_mean)\n",
    "\n",
    "        m_full = Xtr.groupby(c)[target].mean()\n",
    "        Xva[f\"{c}__te\"] = Xva[c].map(m_full).fillna(global_mean)\n",
    "        Xte[f\"{c}__te\"] = Xte[c].map(m_full).fillna(global_mean)\n",
    "\n",
    "        if noise_std > 0:\n",
    "            Xtr[f\"{c}__te\"] += np.random.normal(0, noise_std, size=Xtr.shape[0])\n",
    "\n",
    "    # Return only the newly created TE columns\n",
    "    te_cols = [f\"{c}__te\" for c in cat_cols]\n",
    "    return Xtr[te_cols], Xva[te_cols], Xte[te_cols]\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    Xtr_te, Xva_te, Xte_te = target_mean_encode_oof(df_train[[*cat_cols, TARGET_COL]], \n",
    "                                                     df_valid[[*cat_cols, TARGET_COL]], \n",
    "                                                     df_test[[*cat_cols, TARGET_COL]],\n",
    "                                                     cat_cols=cat_cols, target=TARGET_COL, n_splits=5, noise_std=0.0)\n",
    "    # Combine with numeric (median impute)\n",
    "    num_imp = SimpleImputer(strategy='median')\n",
    "    Xtr_num = pd.DataFrame(num_imp.fit_transform(df_train[num_cols]), columns=num_cols, index=df_train.index)\n",
    "    Xva_num = pd.DataFrame(num_imp.transform(df_valid[num_cols]),   columns=num_cols, index=df_valid.index)\n",
    "    Xte_num = pd.DataFrame(num_imp.transform(df_test[num_cols]),    columns=num_cols, index=df_test.index)\n",
    "\n",
    "    Xtr_all = pd.concat([Xtr_num, Xtr_te], axis=1)\n",
    "    Xva_all = pd.concat([Xva_num, Xva_te], axis=1)\n",
    "    Xte_all = pd.concat([Xte_num, Xte_te], axis=1)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Encode: Target Mean (OOF) + numeric\", Xtr_all, ytr, Xva_all, yva, Xte_all, yte, results_df)\n",
    "else:\n",
    "    print(\"No categorical columns for Target/Mean Encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24409c3",
   "metadata": {},
   "source": [
    "### Frequency / Count Encoding -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(cat_cols) > 0:\n",
    "    Xtr = df_train[num_cols].copy()\n",
    "    Xva = df_valid[num_cols].copy()\n",
    "    Xte = df_test[num_cols].copy()\n",
    "\n",
    "    for c in cat_cols:\n",
    "        freq = df_train[c].value_counts()\n",
    "        Xtr[f\"{c}__freq\"] = df_train[c].map(freq).fillna(0)\n",
    "        Xva[f\"{c}__freq\"] = df_valid[c].map(freq).fillna(0)\n",
    "        Xte[f\"{c}__freq\"] = df_test[c].map(freq).fillna(0)\n",
    "\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "    Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Encode: Frequency/Count\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n",
    "else:\n",
    "    print(\"No categorical columns for Frequency Encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd688d",
   "metadata": {},
   "source": [
    "## 8) Transformations & Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09a681",
   "metadata": {},
   "source": [
    "### Log Transform (log1p with safe shift) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abad4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr = df_train[num_cols].copy()\n",
    "Xva = df_valid[num_cols].copy()\n",
    "Xte = df_test[num_cols].copy()\n",
    "\n",
    "for X in [Xtr, Xva, Xte]:\n",
    "    for c in X.columns:\n",
    "        X[c] = safe_log1p(X[c])\n",
    "\n",
    "imp = SimpleImputer(strategy='median')\n",
    "Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Transform: log1p (safe shift) + median impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4e4db",
   "metadata": {},
   "source": [
    "### Square-root Transform -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr = df_train[num_cols].copy()\n",
    "Xva = df_valid[num_cols].copy()\n",
    "Xte = df_test[num_cols].copy()\n",
    "\n",
    "# Ensure non-negativity\n",
    "shift = 0.0\n",
    "min_val = pd.concat([Xtr.min(), Xva.min(), Xte.min()], axis=1).min(axis=1).min()\n",
    "if min_val < 0:\n",
    "    shift = -min_val\n",
    "\n",
    "for X in [Xtr, Xva, Xte]:\n",
    "    for c in X.columns:\n",
    "        X[c] = np.sqrt(np.maximum(0, X[c] + shift))\n",
    "\n",
    "imp = SimpleImputer(strategy='median')\n",
    "Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Transform: sqrt (with shift if needed) + median impute\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedce49",
   "metadata": {},
   "source": [
    "### Box–Cox (positive) & Yeo–Johnson (any real) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Box–Cox requires strictly positive features; we'll drop non-positive columns for Box–Cox,\n",
    "# then compare with Yeo–Johnson on all numerics.\n",
    "\n",
    "# --- Box–Cox ---\n",
    "pos_cols = [c for c in num_cols if (df_train[c] > 0).all() and (df_valid[c] > 0).all() and (df_test[c] > 0).all()]\n",
    "if len(pos_cols) > 0:\n",
    "    pt_boxcox = PowerTransformer(method='box-cox', standardize=False)\n",
    "    Xtr = pd.DataFrame(pt_boxcox.fit_transform(df_train[pos_cols]), columns=pos_cols, index=df_train.index)\n",
    "    Xva = pd.DataFrame(pt_boxcox.transform(df_valid[pos_cols]),    columns=pos_cols, index=df_valid.index)\n",
    "    Xte = pd.DataFrame(pt_boxcox.transform(df_test[pos_cols]),     columns=pos_cols, index=df_test.index)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Transform: Box–Cox (positive cols only)\", Xtr, ytr, Xva, yva, Xte, yte, results_df)\n",
    "else:\n",
    "    print(\"No strictly positive numeric columns for Box–Cox.\")\n",
    "\n",
    "# --- Yeo–Johnson ---\n",
    "pt_yj = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "Xtr = pd.DataFrame(pt_yj.fit_transform(df_train[num_cols]), columns=num_cols, index=df_train.index)\n",
    "Xva = pd.DataFrame(pt_yj.transform(df_valid[num_cols]),    columns=num_cols, index=df_valid.index)\n",
    "Xte = pd.DataFrame(pt_yj.transform(df_test[num_cols]),     columns=num_cols, index=df_test.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Transform: Yeo–Johnson (all numeric)\", Xtr, ytr, Xva, yva, Xte, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3396b9",
   "metadata": {},
   "source": [
    "### Z-score Standardization -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "imp = SimpleImputer(strategy='median')\n",
    "\n",
    "Xtr = pd.DataFrame(imp.fit_transform(df_train[num_cols]), columns=num_cols, index=df_train.index)\n",
    "Xva = pd.DataFrame(imp.transform(df_valid[num_cols]),   columns=num_cols, index=df_valid.index)\n",
    "Xte = pd.DataFrame(imp.transform(df_test[num_cols]),    columns=num_cols, index=df_test.index)\n",
    "\n",
    "Xtr_s = pd.DataFrame(scaler.fit_transform(Xtr), columns=num_cols, index=df_train.index)\n",
    "Xva_s = pd.DataFrame(scaler.transform(Xva),    columns=num_cols, index=df_valid.index)\n",
    "Xte_s = pd.DataFrame(scaler.transform(Xte),    columns=num_cols, index=df_test.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Scale: Z-score (median impute)\", Xtr_s, ytr, Xva_s, yva, Xte_s, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf86f6",
   "metadata": {},
   "source": [
    "### Min–Max Scaling -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler()\n",
    "imp = SimpleImputer(strategy='median')\n",
    "\n",
    "Xtr = pd.DataFrame(imp.fit_transform(df_train[num_cols]), columns=num_cols, index=df_train.index)\n",
    "Xva = pd.DataFrame(imp.transform(df_valid[num_cols]),   columns=num_cols, index=df_valid.index)\n",
    "Xte = pd.DataFrame(imp.transform(df_test[num_cols]),    columns=num_cols, index=df_test.index)\n",
    "\n",
    "Xtr_s = pd.DataFrame(scaler.fit_transform(Xtr), columns=num_cols, index=df_train.index)\n",
    "Xva_s = pd.DataFrame(scaler.transform(Xva),    columns=num_cols, index=df_valid.index)\n",
    "Xte_s = pd.DataFrame(scaler.transform(Xte),    columns=num_cols, index=df_test.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Scale: Min–Max (median impute)\", Xtr_s, ytr, Xva_s, yva, Xte_s, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc23218",
   "metadata": {},
   "source": [
    "### Power Transformation (Yeo–Johnson + standardize) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "imp = SimpleImputer(strategy='median')\n",
    "\n",
    "Xtr = pd.DataFrame(imp.fit_transform(df_train[num_cols]), columns=num_cols, index=df_train.index)\n",
    "Xva = pd.DataFrame(imp.transform(df_valid[num_cols]),   columns=num_cols, index=df_valid.index)\n",
    "Xte = pd.DataFrame(imp.transform(df_test[num_cols]),    columns=num_cols, index=df_test.index)\n",
    "\n",
    "Xtr_p = pd.DataFrame(pt.fit_transform(Xtr), columns=num_cols, index=df_train.index)\n",
    "Xva_p = pd.DataFrame(pt.transform(Xva),    columns=num_cols, index=df_valid.index)\n",
    "Xte_p = pd.DataFrame(pt.transform(Xte),    columns=num_cols, index=df_test.index)\n",
    "\n",
    "results_df, _ = evaluate_and_log(\"Transform: Yeo–Johnson + standardize\", Xtr_p, ytr, Xva_p, yva, Xte_p, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acca44",
   "metadata": {},
   "source": [
    "## 9) Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e85ea",
   "metadata": {},
   "source": [
    "### Equal-width Bins (on top-variance feature) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(num_cols) > 0:\n",
    "    top = df_train[num_cols].var().sort_values(ascending=False).index[0]\n",
    "    bins = 5\n",
    "    # Fit bins on train\n",
    "    cut_tr, edges = pd.cut(df_train[top], bins=bins, retbins=True, include_lowest=True)\n",
    "    def apply_bins(s, edges):\n",
    "        return pd.cut(s, bins=edges, include_lowest=True, labels=False, duplicates='drop')\n",
    "\n",
    "    Xtr = df_train[num_cols].copy()\n",
    "    Xva = df_valid[num_cols].copy()\n",
    "    Xte = df_test[num_cols].copy()\n",
    "\n",
    "    Xtr[f\"{top}__ewbin\"] = apply_bins(df_train[top], edges)\n",
    "    Xva[f\"{top}__ewbin\"] = apply_bins(df_valid[top], edges)\n",
    "    Xte[f\"{top}__ewbin\"] = apply_bins(df_test[top],  edges)\n",
    "\n",
    "    # Impute then one-hot bin\n",
    "    num_imp = SimpleImputer(strategy='median')\n",
    "    Xtr_n = pd.DataFrame(num_imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xva_n = pd.DataFrame(num_imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "    Xte_n = pd.DataFrame(num_imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "    for X in [Xtr_n, Xva_n, Xte_n]:\n",
    "        X[f\"{top}__ewbin\"] = X[f\"{top}__ewbin\"].fillna(-1).astype(int)\n",
    "    Xtr_all = pd.get_dummies(Xtr_n, columns=[f\"{top}__ewbin\"], drop_first=False)\n",
    "    Xva_all = pd.get_dummies(Xva_n, columns=[f\"{top}__ewbin\"], drop_first=False)\n",
    "    Xte_all = pd.get_dummies(Xte_n, columns=[f\"{top}__ewbin\"], drop_first=False)\n",
    "\n",
    "    Xva_all = Xva_all.reindex(columns=Xtr_all.columns, fill_value=0)\n",
    "    Xte_all = Xte_all.reindex(columns=Xtr_all.columns, fill_value=0)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Bin: Equal-width (5) on top-var\", Xtr_all, ytr, Xva_all, yva, Xte_all, yte, results_df)\n",
    "else:\n",
    "    print(\"No numeric columns for binning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa899d",
   "metadata": {},
   "source": [
    "### Quantile Bins (q=5 on top-variance feature) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(num_cols) > 0:\n",
    "    top = df_train[num_cols].var().sort_values(ascending=False).index[0]\n",
    "    q = 5\n",
    "    # Fit qcut on train\n",
    "    q_tr, edges = pd.qcut(df_train[top], q=q, retbins=True, duplicates='drop')\n",
    "    def apply_qbins(s, edges):\n",
    "        return pd.cut(s, bins=edges, include_lowest=True, labels=False, duplicates='drop')\n",
    "\n",
    "    Xtr = df_train[num_cols].copy()\n",
    "    Xva = df_valid[num_cols].copy()\n",
    "    Xte = df_test[num_cols].copy()\n",
    "\n",
    "    Xtr[f\"{top}__qbin\"] = apply_qbins(df_train[top], edges)\n",
    "    Xva[f\"{top}__qbin\"] = apply_qbins(df_valid[top], edges)\n",
    "    Xte[f\"{top}__qbin\"] = apply_qbins(df_test[top],  edges)\n",
    "\n",
    "    num_imp = SimpleImputer(strategy='median')\n",
    "    Xtr_n = pd.DataFrame(num_imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xva_n = pd.DataFrame(num_imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "    Xte_n = pd.DataFrame(num_imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "    for X in [Xtr_n, Xva_n, Xte_n]:\n",
    "        X[f\"{top}__qbin\"] = X[f\"{top}__qbin\"].fillna(-1).astype(int)\n",
    "    Xtr_all = pd.get_dummies(Xtr_n, columns=[f\"{top}__qbin\"], drop_first=False)\n",
    "    Xva_all = pd.get_dummies(Xva_n, columns=[f\"{top}__qbin\"], drop_first=False)\n",
    "    Xte_all = pd.get_dummies(Xte_n, columns=[f\"{top}__qbin\"], drop_first=False)\n",
    "\n",
    "    Xva_all = Xva_all.reindex(columns=Xtr_all.columns, fill_value=0)\n",
    "    Xte_all = Xte_all.reindex(columns=Xtr_all.columns, fill_value=0)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Bin: Quantile (q=5) on top-var\", Xtr_all, ytr, Xva_all, yva, Xte_all, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ff094",
   "metadata": {},
   "source": [
    "### Binary Binning (high/low by median of top-variance feature) -> LR + RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a3c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(num_cols) > 0:\n",
    "    top = df_train[num_cols].var().sort_values(ascending=False).index[0]\n",
    "    med = df_train[top].median()\n",
    "\n",
    "    Xtr = df_train[num_cols].copy()\n",
    "    Xva = df_valid[num_cols].copy()\n",
    "    Xte = df_test[num_cols].copy()\n",
    "\n",
    "    Xtr[f\"{top}__high\"] = (df_train[top] > med).astype(int)\n",
    "    Xva[f\"{top}__high\"] = (df_valid[top] > med).astype(int)\n",
    "    Xte[f\"{top}__high\"] = (df_test[top]  > med).astype(int)\n",
    "\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    Xtr_i = pd.DataFrame(imp.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xva_i = pd.DataFrame(imp.transform(Xva),   columns=Xva.columns, index=Xva.index)\n",
    "    Xte_i = pd.DataFrame(imp.transform(Xte),   columns=Xte.columns, index=Xte.index)\n",
    "\n",
    "    results_df, _ = evaluate_and_log(\"Bin: Binary (median split) on top-var\", Xtr_i, ytr, Xva_i, yva, Xte_i, yte, results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16003644",
   "metadata": {},
   "source": [
    "## 10) Final Comparison Table + Quick Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44231ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort by validation RMSE (ascending)\n",
    "display_cols = [\"method\",\"n_features_after_prep\",\"rmse_train\",\"rmse_valid\",\"rmse_test\",\"mae_valid\",\"r2_valid\"]\n",
    "results_sorted = results_df.sort_values(by=\"rmse_valid\", ascending=True)[display_cols].reset_index(drop=True)\n",
    "print(\"Best (by Valid RMSE):\")\n",
    "print(results_sorted.head(10))\n",
    "\n",
    "# (Optional) Save results to CSV\n",
    "results_sorted.to_csv(\"assignment1_results.csv\", index=False)\n",
    "print(\"\\nSaved: assignment1_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65057be",
   "metadata": {},
   "source": [
    "\n",
    "### Short Commentary (fill after running)\n",
    "- Which methods helped most on **valid RMSE**? Why might they help for your dataset?\n",
    "- Did any methods **overfit** (big train–valid gap)?\n",
    "- How did **encodings** compare (label vs one-hot vs target)?\n",
    "- Which **transform/scale** worked best with LR?\n",
    "- Any **data leakage** pitfalls you avoided (fitting on train only, OOF encodings)?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
